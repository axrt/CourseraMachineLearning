---
title: "Lecture 2"
author: "Alexander Tuzhikov"
date: "November 12, 2015"
output: 
  html_document: 
    fig_caption: yes
    fig_height: 9
    fig_width: 12
    highlight: haddock
    keep_md: yes
    number_sections: yes
    theme: cosmo
    toc: yes
---

#Caret functionality

* Some preprocessing (cleaning)
    * `preProcess`
* Data splitting
    * `createDataPartition`
    * `createResample`
    * `createTimeSlices`
* Training/testing funcitions
    * `train`
    * `predict`
* Model comparison
    * `confusionMatrix`
    
`caret` allows to unify the parameters for a wide variaty of predicting models.

```{r, warning=FALSE}
library(caret)
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
set.seed(32343)
modelFit <- train(type ~., data = training, method="glm")
modelFit$finalModel
predictions <- predict(modelFit, newdata=testing)
predictions
confusionMatrix(predictions, testing$type)
```

#Data slicing

```{r, warning=FALSE}
set.seed(32323)
folds<- createFolds(y=spam$type, k=10, list=TRUE, returnTrain=TRUE)
sapply(folds, length)
folds[[1]][1:10]
set.seed(32323)
folds<- createResample(y=spam$type, times=10, list=TRUE)
sapply(folds, length)
folds[[1]][1:10]
set.seed(32323)
tme<- 1:1e3
folds<- createTimeSlices(y=tme, initialWindow = 20, horizon = 10) #horizon is the number of points you are going to be predicting
names(folds)
folds$train[[1]]
folds$test[[1]]
```

#Training options

```{r, warning=FALSE}
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
set.seed(32343)
modelFit <- train(type ~., data = training, method="glm")
args(train.default)
#needs trainControls()
```

##Continous outcomes:  

* RMSE = Root Mean Squared Error
* RSquared = R^2 from regression models

##Categorical outcomes:  

* Accurcy - Fraction correct
* Kappa - A measure of concordance

##`trainControl` resampling

* `method`

    * boot = bootstrapping
    * boot632 = bootstrapping with adjustment
    * cv = cross validation
    * repeatedcv = repeated cross validation
    * LOOCV = leave one out cross validation

* `number`
    
    * For boot/cross validation
    * Number of subsamples to take
    
* `repeats`

    * Number of times to repeate subsampling
    * if big this can slow things down

###Setting the seed

* it is often useful to set an overall seed
* you can also set a seed for each resample
* seeding each resample is useful for parallel fits

#Plotting predictors

```{r, warning=FALSE}
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training<- Wage[inTrain,]
testing<- Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x=training[,c("age", "education", "jobclass")], y=training$wage, plot="pairs")
qplot(age, wage, data=training)
qplot(age, wage, colour=jobclass, data=training)
qplot(age, wage, colour=education, data=training) + geom_smooth(method="lm", formula=y~x)
library(Hmisc)
library(pander)
cutWage<- cut2(training$wage, g=3)
pander(table(cutWage))
q1<- qplot(cutWage, age, data=training, fill=cutWage, geom=c("boxplot"))
q2<- qplot(cutWage, age, data=training, fill=cutWage, geom=c("boxplot", "jitter"))
library(grid)
library(gridExtra)
grid.arrange(q1, q2, ncol=2)

t1<- table(cutWage, training$jobclass)
pander(t1)
pander(prop.table(t1,1))#proportion in each row, 2 would have been column
qplot(wage, colour=education, data=training, geom="density")
```

##Notes

* Make your plots only in trainig set
    * don't use the test set for exploration!
* Things you should be looking for 
    * imbalance in outcomes/predictors
    * outliers
    * groups of points not explained by a predictor
    * skewed variables

#Preprocessing

```{r, warning=FALSE}
library(caret)
library(kernlab)
data("spam")
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve, main = "", xlab = "ave. capital run length")
mean(training$capitalAve)
sd(training$capitalAve)
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS)
sd(trainCapAveS)
preObj<- preProcess(training[, -58], method = c("center", "scale"))
trainCapAveS<- predict(preObj, training[-58])$capitalAve
testCapAveS <- predict(preObj, testing[,-58])$capitalAve
mean(testCapAveS)
sd(testCapAveS)
set.seed(32343)
modelFit<- train(type ~., data=training,
                 preProcess=c("center","scale"),
                 method="glm")
modelFit
#removing suspiciously variable or very volatile predictors
preObj<- preProcess(training[,-58], method = c("BoxCox"))
trainCapAveS <- predict(preObj, training[,-58])$capitalAve
par(mfrow=c(1,2))
hist(trainCapAveS)
qqnorm(trainCapAveS)

set.seed(13343)
#Make some values NA
training$capAve<- training$capitalAve
selectNA<- rbinom(dim(training)[1], size=1, prob=0.05)==1
training$capAve[selectNA]<- NA

#Impute and standardize
preObj<- preProcess(training[,-58], method="knnImpute")
capAve<- predict(preObj, training[,-58])$capAve

#Standardize true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
```

* Traning and tests must be processed in the same way
* Test transformations will likely be imperfect
    * especially if the test/training sets collected at different times
* Careful when transforming factor varibles!

#Covariate creation

1. Level 1: From raw data to covariate
2. Level 2: Transforming tidy covariates


```{r, warning=FALSE}
library(kernlab)
data(spam)
spam$capitalAveSq<- spam$capitalAve^2
```

##Level 1, Raw data -> covariates

* Depends heavily on an application
* The balancing act is summarization vs. infromation loss
* Examples:
    * text files: frequency of words, frequency of phrases (Google ngrams), frequency of capital letters.
    * images: edges, corners, blobs, ridges (computer vision, feature detection)
    * webpages: number and type of images, position of elements, colors, videos (A/B Testing)
* The more knowledge of the system you have the better the job you will do.
* When in doubt, err on the side of more features
* Can be automated, but use caution!

##Level 2, Tidy covariates -> new covariates

* More necessary for some methods (regression, svms) that for others (classification trees)
* Should be done *only* on the training set
* The best approach is through exploratory analysis (plotting and tables)
* New covariates should be added to data frames

```{r, warning=FALSE}
library(ISLR)
library(caret)
data("Wage")
inTrain<- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training<- Wage[inTrain,]
testing<- Wage[-inTrain,]
```

Basic idea is to turn factor variables into indicator variables.

```{r, warning=FALSE}
table(training$jobclass)
dummies<- dummyVars(wage ~ jobclass, data=training)
head(predict(dummies, newdata=training))

nsv<- nearZeroVar(training, saveMetrics = TRUE)
nsv

library(splines)
bsBasis <- bs(training$age, df=3)
bsBasis

lm1<- lm(wage ~ bsBasis, data=training)
plot(training$age, training$wage, pch=19, cex=0.5)
points(training$age, predict(lm1, newdata=training), col="red", pch=19, cex=0.5)

predict(bsBasis, age=testing$age)
```
##Notes and further reading

* Level 1 frature creation (raw data to covariates)
    * Science is key. Google "feature extraction for [data type]"
    * Err on overcreation of features
    * in some applications (images, voices) automated feature creation is possible/necessary
    * *that link from the lecture*
* Level 2 feature creation (covariates to new covariates)
    * The function `preProcess` in `caret` will handle some preprocessing
    * Create new covariates if you think they will improve the fit
    * Use exploratory analysis on the training set for creating them
    * Be careful about overfitting!
* If you want tot fit spline models, use the `gam` method in the `caret` package which allows smoothing of multiple variables
* More on feature creation/data tidying in the Obtaining Data course from the Data Science (done already)

#Preprocessing with principle components analysis

```{r, warning=FALSE}
library(caret)
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]

M<- abs(cor(training[,-58]))
diag(M) <-0
which(M>0.8, arr.ind=T)
names(spam)[c(34, 32)]
plot(spam[, 34], spam[,32])
```

##Basic PCA idea

* We might not need every predictor
* A weighted combination of predictors might be better
* We should pick this combination to capture the "most information" possible
* Benefits
    * Reduced number of predictors
    * Reduced noise (due to averaging)

```{r, warning=FALSE}
X<- 0.71*training$num415 + 0.71* training$num857
Y<- 0.71*training$num415 - 0.71* training$num857
plot(X, Y)
```

##Related problems

You have multivariate X1 ... Xn so X1=(X11 ... Xm)

* Find a new set of multivariate variables that are uncorrelated and explain as much variace as possible
* If you put all the variables together in one matrix, find the best matrix created with fewer varibales (lower rank) that explains the original data

The first goal is statistical and the second is data compression

###SVD

If X is a matrix with each variable in a column and each observation is a row then the SVD is a "matrix decompression"

X=UDV#T

where the column of U are orthogonal (left singular vectors), the columns of V are orthogonal (right singular vectors) and D is a diagonal matrix (singular values)

###PCA

The principal components are equal to the right singular values if you first scale (substract the mean, devide by standard deviation) the variables.

```{r, warning=FALSE}
smallSpam <- spam[,c(34, 32)]
prComp<- prcomp(smallSpam)
plot(prComp$x[,1], prComp$x[,2])
prComp$rotation

typeColor <- ((spam$type=="spam")*1+1)
prComp<- prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1], prComp$x[,2], col=typeColor, xlab="PC1", ylab="PC2")

preProc <- preProcess(log10(spam[,-58]+1), method="pca", pcaComp = 2)
spamPC <- predict(preProc, log10(spam[,-58]+1))
plot(spamPC[, 1], spamPC[,2], col=typeColor)
trainPC <- predict(preProc, log10(training[, -58]+1))
modelFit<- train(training$type ~., method="glm", data=trainPC)
testPC<- predict(preProc, log10(testing[, -58]+1))
confusionMatrix(testing$type, predict(modelFit, testPC))

modelFit<- train(training$type ~., method="glm", preProcess="pca", data=training)
confusionMatrix(testing$type, predict(modelFit, testing))
```

* Most useful for linear-type models
* Can make it harder to interpret predictors
* Watch for ouliers!
    * Transform first (with logs/BoxCox)
    * Plot predictors to identify problems
For more see the "Elements of Statistical Learning" (have started reading, stopped at logistic regression)

#Predicting with regression

* fit a simple model
* plug in new covariates and multiply by the coefficients
* useful when the linear model is (nearly correct)

Pros: 

* Easy to implement
* Easy to interpret

Cons:

* Often poor performance in nonlinear settings

```{r, warning=FALSE}
library(caret)
data("faithful")
set.seed(333)

inTrain<- createDataPartition(y=faithful$waiting, p=0.5, list=FALSE)
trainFaith<- faithful[inTrain,]
testFaith<- faithful[-inTrain,]
head(trainFaith)

plot(trainFaith$waiting, trainFaith$eruptions, pch=19, col="blue", xlab="Waiting", ylab="Duration")

lm1<- lm(eruptions ~ waiting, data=trainFaith)
summary(lm1)
lines(trainFaith$waiting, lm1$fitted.values, lwd=3)
coef(lm1)[1] + coef(lm1)[2]*0.8
newdata<- data.frame(waiting=80)
predict(lm1, newdata)

par(mfrow=c(1,2))
plot(trainFaith$waiting, trainFaith$eruptions, pch=19, col="blue", xlab="Waiting", ylab="Duration")
lines(trainFaith$waiting, predict(lm1), lwd=3)
plot(testFaith$waiting, testFaith$eruptions,pch=19, col="blue", xlab="Waiting", ylab="Duration")
lines(testFaith$waiting, predict(lm1, newdata=testFaith), lwd=3)
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
sqrt(sum((predict(lm1, newdata=testFaith)-testFaith$eruptions)^2))

pred1 <- predict(lm1, newdata = testFaith, interval="prediction")
ord<- order(testFaith$waiting)
plot(testFaith$waiting, testFaith$eruptions, pch=19, col="blue")
matlines(testFaith$waiting[ord], pred1[ord,], type="l", col=c(1,2,2), lty=c(1,1,1), lwd=3)

modFit<- train(eruptions~waiting, data=trainFaith, method="lm")
summary(modFit$finalModel)
```

* Regression models with multiple covariates can be included
* Often useful in combination with other models

#Prediction with regression, multiple covariates

```{r, warning=FALSE}
library(ISLR)
library(ggplot2)
library(caret)
data("Wage")
Wage<- subset(Wage, select=-c(logwage))
summary(Wage)

inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training<- Wage[inTrain,]
testing<- Wage[-inTrain,]
dim(training)
dim(testing)

featurePlot(x=training[, c("age", "education", "jobclass")],
            y=training$wage, plot="pairs")

qplot(age, wage, data=training)
qplot(age, wage, colour=jobclass, data=training)
qplot(age, wage, colour=education, data=training)

modFit<- train(wage~age+jobclass+education, method="lm", data=training)
finMod<- modFit$finalModel
print(modFit)
plot(finMod, 1, pch=19, cex=0.5, col="#00000010")
qplot(finMod$fitted.values, finMod$residuals, colour=race, data=training)
plot(finMod$residuals, pch=19) #some variables are missing from the model and/or data
pred<- predict(modFit, testing)
qplot(wage, pred, colour=year, data=testing)

modFitAll<- train(wage~., data=training, method="lm")
pred<- predict(modFitAll, testing)
qplot(wage, pred, data=testing)
```

* Often use in combination with other models
* Read stuff on the above






