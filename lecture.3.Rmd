---
title: "Lecture 3"
author: "Alexander Tuzhikov"
date: "November 16, 2015"
output: 
  html_document: 
    fig_caption: yes
    fig_height: 9
    fig_width: 12
    highlight: haddock
    keep_md: yes
    number_sections: yes
    theme: cosmo
    toc: yes
---

#Predicting with trees

* Iteratively split variables into groups
* Evaluate "homogenity" within each group
* Split again if necessary

##Pros:

* Easy to interpret
* Better performance in nonlinear settings

##Cons:

* Without pruning/cross-validation can lead to overfitting
* Harder to estimate uncertaintly
* Results may be variable

##Algorithm

1. Start with all variables in one group
2. Find the variable/split that best separates the outcomes
3. Divide the data into two groups ("leaves") on that split ("node")
4. Within each split, find the best variable/split that separates the outcomes
5. Continue until the groups are too small or sufficiently "pure"

```{r}
data("iris")
library(ggplot2)
library(caret)
names(iris)
table(iris$Species)

inTrain<- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training<- iris[inTrain,]
testing<- iris[-inTrain,]
dim(training)
dim(testing)

qplot(Petal.Width, Sepal.Width, colour=Species, data=training)

modFit<- train(Species ~ ., method="rpart", data=training)
print(modFit$finalModel)

plot(modFit$finalModel, uniform=TRUE, main= "Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)

library(rattle)
library(rpart)
library(rpart.plot)
predict(modFit, newdata=testing)
fancyRpartPlot(modFit$finalModel)
```

* Classification trees are non-linear models
    * they use interactions between variables
    * data transformations my be less importatant (monotone transformations)
    * Threes can also be used for regression problems (continous outcome)
* Note that there are multiple tree building options in R both in the `caret` package - `part`, `rpart` and out of the `caret` package - `tree`

Book: "Calssification and regression trees"

#Bagging (bootstrap aggregating)

##Basic idea:

1. Take your dataset and resample cases, recalculate predictions
2. Average or majority vote

##Notes:

* Similar bias
* Reduced variance
* More useful for non-linear functions

```{r, warning=FALSE}
library(ElemStatLearn)
data("ozone",package = "ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)

ll<- matrix(NA, nrow=10, ncol=155)
for(i in 1:10){
        ss<- sample(1:dim(ozone)[1], replace=TRUE)
        ozone0<- ozone[ss,]
        ozone0<- ozone0[order(ozone0$ozone),]
        loess0<- loess(temperature ~ ozone, data=ozone0, span=0.2)
        ll[i,]<- predict(loess0, newdata=data.frame(ozone=1:155))
}

plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
for(i in 1:10){
        lines(1:155, ll[i,], col="grey", lwd=2)
}
lines(1:155, apply(ll, 2, mean), col="red", lwd=2)
```

##Bagging in caret

* Some models perform bagging for you, in `train` function consider `method` options:
    * `bagEarth`
    * `treebag`
    * `bagFDA`
* Alternatively you can bag any model you choose using the `bag` function

##Creating a roll-your-own

```{r, warning=FALSE}
predictors <- data.frame(ozone=ozone$ozone)
temperature<- ozone$temperature
treebag <- bag(predictors, temperature, B=10, bagControl = bagControl(fit=ctreeBag$fit, predict = ctreeBag$pred, aggregate = ctreeBag$aggregate))
```

[source](http://www.inside-r.org/packages/cran/docs/nbBag)

```{r, warning=FALSE}
plot(ozone$ozone, temperature, col="lightgrey", pch=19)
points(ozone$ozone, predict(treebag$fits[[1]]$fit, predictors), pch=19, col="red")
points(ozone$ozone, predict(treebag, predictors), pch=19, col="blue")

ctreeBag$fit

```

##Notes:

* Bagging is most useful for nonlinear models
* Often used with trees - an extension is random forests
* Several models use bagging in `caret`'s `train` function

Read the links about Bagging and Boosting

#Random Forests

1. Bootstrap samples
2. At each split, bootstrap variables
3. Grow multiple trees and vote

##Pros

1. Accuracy

##Cons

1. Speed
2. Interpretability
3. Overfitting

```{r, warning=FALSE}
data(iris)
library(ggplot2)

inTrain<- createDataPartition(y=iris$Species, p=0.7, list=FALSE)

training<- iris[inTrain,]
testing<- iris[-inTrain,]

library(caret)
library(randomForest)
modFit <- train(Species~.,data=training, method="rf", prox=TRUE)
modFit
getTree(modFit$finalModel,k=2)
irisP <- classCenter(training[, c(3,4)], training$Species, modFit$finalModel$prox)
irisP<- as.data.frame(irisP)
irisP$Species<- rownames(irisP)
p<- qplot(Petal.Width, Petal.Length, col=Species, data=training)
p+ geom_point(aes(x=Petal.Width, y=Petal.Length, col=Species), size=5, shape=4, data=irisP)

pred<- predict(modFit, testing)
testing$predRight<- pred==testing$Species
table(pred, testing$Species)

qplot(Petal.Width, Petal.Length, colour=predRight, data=testing, main = "newdata Predictions")
```

##Notes

* Random forests are usually one of the two top performing algorithms along with boosting in prediction contests
* Random forests are diffictult to interpret but often very accurate
* Care should be taken to avoid overfitting (see `rfcv`) function

#Boosting

1. Take lots of (possibly) weak predictors
2. Weight them and add them up
3. Get a stronge predictor

1. Start with a set of calssifiers h1...hk
    *Examples: all possible trees, all possible regression models, all possible cutoffs
    
2. Create a classifier that combines classification functions: f(x)=sgn(sum(at*ht(x)))
    * Goal is to minimize error (on training set)
    * Iterative, select one h at each step
    * Calculate weights based on errors
    * Upweight missed classificaitons and select next h
    
[Some suggested link](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)
[Adaboost](https://en.wikipedia.org/wiki/AdaBoost)

* Boosting can be used with any subset of classifiers
* One large subclass is gradient boosting
* R has multiple boosting libraries. Differences include the choice of basic classification functions and combination rules.
    * `gbm` - boosting with trees
    * `mboost` - model based boosting
    * `ada` - statistical boosting based on additive logistic regression
    * `gamBoost` - for boosting generalized additive models
* Most of these are available in the caret package

```{r, warning=FALSE}
library(ISLR)
data(Wage)
library(ggplot2)
library(caret)

Wage <- subset(Wage, select=-c(logwage))
inTrain<- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)

training<- Wage[inTrain,]
testing<- Wage[-inTrain,]

modFit<- train(wage ~., method="gbm", data=training, verbose=FALSE)
modFit

qplot(predict(modFit, testing), wage, data=testing)
```

* Ron Meir
* Freund and Shapire
* Slides

#Model based prediction

1. Assume that the data follow a probabilistic model
2. Use Bayes' theorem to identify optimal classifiers

##Pros

* Can take advantage of the structure of the data
* May be computationally convinient
* Are reasonably accurate on real problems

##Cons

* Make additional assumptions about the data
* When the model is incorrect you may get reduced accuracy

A range of models use this approach

* Linear discriminant analysis assumes fk(x) is multivariate Gaussian with same covariances
* Quadratic discriminant alanysys assumes fk(x) is multivariate Gaussian with different covariances
* Model based prediction assumes more complicated version for the covariance matrix
* Naive Bayes assumes independence between features for model building

```{r, warning=FALSE}
data(iris)
library(ggplot2)
names(iris)
table(iris$Species)
inTrain<- createDataPartition(y=iris$Species, p=0.7, list=FALSE)

training<- iris[inTrain,]
testing<- iris[-inTrain,]

modlda<- train(Species ~., data=training, method="lda")
modnb <- train(Species ~., data=training, method="nb")
plda <- predict(modlda, testing)
pnb<- predict(modnb, testing)
table(plda, pnb)
equalPredictions <- (plda==pnb)
qplot(Petal.Width, Sepal.Width, colour=equalPredictions, data=testing)
```

Read wikipedia and the books